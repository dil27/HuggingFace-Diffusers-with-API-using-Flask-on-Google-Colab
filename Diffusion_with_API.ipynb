{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM56u8qiEo2dzvL99spAjw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dil27/HuggingFace-Diffusers-with-API-using-Flask-on-Google-Colab/blob/main/Diffusion_with_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mY-ckOlWYQYA"
      },
      "outputs": [],
      "source": [
        "#@markdown #Installation.\n",
        "#@markdown Run this cell. No need to configure, I'm serious.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown This notebook is already built-in **EasyNegative** and with default **DPM++ 2M Karras** sampling method.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown If you're accidentally stop the cells, just start the **second** runtume. You don't need to run this cell again.\n",
        "!pip install diffusers[\"torch\"] transformers\n",
        "!pip install accelerate\n",
        "!pip install git+https://github.com/huggingface/diffusers\n",
        "!pip install Flask pyngrok\n",
        "!pip install flask-cors\n",
        "!pip install peft safetensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from PIL import Image\n",
        "from flask_cors import CORS\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "\n",
        "#@markdown #Setup\n",
        "#@markdown --- Get your _ngrok_ token [here](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "\n",
        "NGROK_TOKEN = \"<your_ngrok_token_here>\" #@param {type:\"string\"}\n",
        "!ngrok authtoken $NGROK_TOKEN\n",
        "\n",
        "#@markdown --- After running this cell, you will get your runtime url in console\n",
        "\n",
        "#@markdown example: `https://8fc2-34-125-49-157.ngrok-free.app/`\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "pipe = None\n",
        "checkpoint = None\n",
        "\n",
        "@app.route('/connect', methods=['POST'])\n",
        "def connect():\n",
        "    return jsonify({\n",
        "        \"msg\": \"Connected\"\n",
        "    })\n",
        "\n",
        "@app.route('/loadcheckpoint', methods=['POST'])\n",
        "def loadCheckpoint():\n",
        "    global pipe\n",
        "    global checkpoint\n",
        "    data = request.get_json()\n",
        "    checkpoint = data.get('checkpoint')\n",
        "\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(checkpoint, torch_dtype=torch.float16)\n",
        "    pipe = pipe.to('cuda')\n",
        "    pipe.safety_checker = None\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe.load_textual_inversion(\n",
        "        \"embed/EasyNegative\",\n",
        "        weight_name=\"EasyNegative.safetensors\",\n",
        "        token=\"EasyNegative\"\n",
        "    )\n",
        "\n",
        "    # pipe.load_lora_weights(\n",
        "    #     \"Shalie/GenshinImpactSigewinne\",\n",
        "    #     weight_name=\"spgiSigewinneXLPony.safetensors\",\n",
        "    #     adapter_name=\"sigewinnedef\"\n",
        "    # )\n",
        "    # pipe.set_adapters(\n",
        "    #     [\n",
        "    #         \"sigewinnedef\"\n",
        "    #     ],\n",
        "    #     adapter_weights=[\n",
        "    #         0.7\n",
        "    #     ]\n",
        "    # )\n",
        "\n",
        "    return jsonify({\n",
        "        \"msg\": \"Checkpoint successfully loaded\",\n",
        "        \"checkpoint\": checkpoint\n",
        "    })\n",
        "\n",
        "@app.route('/checkpipe', methods=['POST'])\n",
        "def checkPipe():\n",
        "    global pipe\n",
        "    global checkpoint\n",
        "    if not pipe:\n",
        "        return jsonify({\"error\":\"Pipeline is not loaded. Please load checkpoint first\"})\n",
        "\n",
        "    return jsonify({\"pipe\": checkpoint})\n",
        "\n",
        "@app.route('/txt2img', methods=['POST'])\n",
        "def generate():\n",
        "  try:\n",
        "    global pipe\n",
        "    global checkpoint\n",
        "    if not pipe:\n",
        "        return jsonify({\"error\":\"Pipeline is not loaded. Please load checkpoint first\"})\n",
        "\n",
        "    data = request.get_json()\n",
        "\n",
        "    prompt     = data.get('prompt')\n",
        "    neg        = data.get('neg')\n",
        "    seed       = data.get('seed')\n",
        "    width      = data.get('width')\n",
        "    height     = data.get('height')\n",
        "    denoise    = data.get('sampling')\n",
        "    guidance   = data.get('guidance')\n",
        "    checkpoint = data.get('checkpoint')\n",
        "\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "    image = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=neg,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        num_inference_steps=denoise,\n",
        "        guidance_scale=guidance,\n",
        "        added_cond_kwargs={\"text_time\": None},\n",
        "        generator=generator,\n",
        "    ).images[0]\n",
        "\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"PNG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "    return jsonify({\n",
        "        \"data\": {\n",
        "            \"checkpoint\": checkpoint,\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": neg,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"denoise\": denoise,\n",
        "            \"guidance\": guidance,\n",
        "            \"seed\": seed\n",
        "        },\n",
        "        \"image\": img_str\n",
        "    })\n",
        "\n",
        "  except Exception as e:\n",
        "      return jsonify({\"error\": str(e)}), 400\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(5000)\n",
        "print(' * Ngrok URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5hHjs9A_YR3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}